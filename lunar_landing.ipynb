{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The objective of this project is to solve the lunar landing game from OpenAI Gym using Deep Q-Learning.\n",
    "\n",
    "For additional details and analysis of lunar_landing.ipynb, please refer to lunar_landing.pdf.\n",
    "\n",
    "Steps to run the uploaded Jupyter Notebook, lunar_landing.ipynb, are shown below. This will create and run the Deep Q-learning algorithm to solve the Lunar Landing game from OpenAI Gym.\n",
    "\n",
    "Run cell 1 to import all the necessary libraries\n",
    "Run cell 2 to setup the neural network\n",
    "Run cell 3 to setup the class and functions to train the model\n",
    "Run cell 4 to train the model\n",
    "Run cell 5 to create a graph to see the rewards per episode during training\n",
    "Run cell 6 to save the model (if necessary)\n",
    "Run cell 7 to load the model (if necessary)\n",
    "Run cell 8 to create a graph using the trained model to see reward per episode'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install box2d-py\n",
    "#!pip3 install gym[Box_2D]\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "batch_size = 32\n",
    "input_size = 8\n",
    "layer1_size = 32\n",
    "layer2_size = 32\n",
    "output_size = 4\n",
    "lr = 0.001\n",
    "replay_size = 10000\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "#simple neural network \n",
    "class network(nn.Module):\n",
    "    def __init__(self, input_size, layer1_size, layer2_size, output_size):\n",
    "        super(network,self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Linear(input_size, layer1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer1_size, layer2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer2_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.cnn(x)\n",
    "\n",
    "    def act(self,results):\n",
    "        #choose option with highest value\n",
    "        act = torch.argmax(results).item()\n",
    "        return act\n",
    "\n",
    "#trianing model that will be updated with each step\n",
    "model = network(input_size, layer1_size, layer2_size, output_size)\n",
    "#target or more stable model used to determine update in each step; only gets updated occasionally\n",
    "target_model = network(input_size, layer1_size, layer2_size, output_size)\n",
    "#both models will start with same parameters\n",
    "target_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use MSE for loss calculation\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "#use Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "class Train:\n",
    "    #initialization\n",
    "    def __init__(self):\n",
    "        self.alpha = 1\n",
    "        self.gamma = 0.6\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.9\n",
    "        self.min_epsilon = 0.01\n",
    "\n",
    "    #update model using replay_memory and Q learning\n",
    "    def train(self,replay_memory,model,loss_fn,optimizer):\n",
    "        x = []\n",
    "        y = []\n",
    "        #randomly sample replay_size amount of steps if replay_memory is greater than or equal to that amount\n",
    "        if len(replay_memory) >= replay_size:\n",
    "          batch = random.sample(replay_memory[-replay_size:],batch_size)\n",
    "        #randomly sample batch_size amount of steps if replay_memory is greater than or equal to that amount\n",
    "        elif len(replay_memory) >= batch_size:\n",
    "          batch = random.sample(replay_memory,batch_size)\n",
    "        #else just use all of the replay_memory since not enough data yet\n",
    "        else:\n",
    "          batch = replay_memory\n",
    "\n",
    "        #current state of each episode \n",
    "        state_current = np.array([item[0] for item in batch])\n",
    "        #next state of each episode\n",
    "        state_next = np.array([item[1] for item in batch])\n",
    "\n",
    "        #get prediction of action using training model and current state\n",
    "        pred_current = model(torch.tensor(state_current))\n",
    "        #get prediction of action using target model and next state\n",
    "        pred_next = target_model(torch.tensor(state_next))\n",
    "\n",
    "        #for each episode in the batch\n",
    "        for i,(state,new_state,action,reward,done) in enumerate(batch):\n",
    "            #when have not reached a terminal state\n",
    "            if not done:\n",
    "                #q(s,a) = reward + gamma * max q(s',a')\n",
    "                fq_max = reward + self.gamma*torch.max(pred_next[i]).item()\n",
    "            #when reached terminal state\n",
    "            else:\n",
    "                #q(s,a) = reward\n",
    "                fq_max = reward\n",
    "\n",
    "            q = pred_current[i]\n",
    "            #update q value for current state and action\n",
    "            q[action] = fq_max\n",
    "\n",
    "            x.append(state) \n",
    "            y.append(q.detach().numpy()) \n",
    "\n",
    "        #update model with loss and backpropagation\n",
    "        model.train()\n",
    "        pred = model(torch.tensor(x))\n",
    "        loss = loss_fn(pred,torch.tensor(y))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #use trained model to predict next steps\n",
    "    def test(self,episodes,target_model,loss_fn,test_reward):\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for ep in episodes:\n",
    "                done = False\n",
    "                score = 0\n",
    "                state = env.reset()\n",
    "                while not done:\n",
    "                    a = target_model.act(target_model(torch.tensor(state)))\n",
    "\n",
    "                    next_state, r, done, _ = env.step(a)\n",
    "\n",
    "                    score += r\n",
    "\n",
    "                    state = next_state\n",
    "                test_reward.append(score)\n",
    "\n",
    "    \n",
    "    #Determine next action for each step and update models\n",
    "    def update(self,model, loss_fn, optimizer,train_rewards):\n",
    "        replay_memory = []\n",
    "        steps_update = 0\n",
    "        episodes = 0\n",
    "\n",
    "        #Continue updates until reach terminal state\n",
    "        while True:\n",
    "            episodes += 1\n",
    "            state = env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                steps_update += 1\n",
    "                #uncomment below line to watch visual\n",
    "                #env.render()\n",
    "\n",
    "                rand_num = np.random.random()\n",
    "                #allow some randomness in choice of action based on if random number selected is less than or equal to epsilon\n",
    "                if rand_num <= self.epsilon:\n",
    "                    a = np.random.randint(4)\n",
    "                else:\n",
    "                  #choose action based on model\n",
    "                  with torch.no_grad():\n",
    "                    results = model(torch.tensor(state))\n",
    "                  a = model.act(results)\n",
    "\n",
    "                #using built in step function for gym, get new state, reward, and if reached terminal state based on action provided\n",
    "                new_state, r, done, _ = env.step(a)\n",
    "\n",
    "                #update overall score with reward value\n",
    "                score += r\n",
    "\n",
    "                #add to the replay memory\n",
    "                replay_memory.append([state,new_state,a,r,done])\n",
    "\n",
    "                #update current state with new state\n",
    "                state = new_state\n",
    "\n",
    "                #update training model after each step once the number of steps is greater than batch_size \n",
    "                #This allows there to be some data before the model is updated\n",
    "                if steps_update >= batch_size:\n",
    "                  Train.train(self,replay_memory,model,loss_fn,optimizer)\n",
    "                \n",
    "                #Every 90 steps, update the target model so it stays more stable than the training model\n",
    "                if steps_update % 90 == 0:\n",
    "                    target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "            train_rewards.append(score)\n",
    "            print('score: {}'.format(score))\n",
    "            #decay epsilon every 20 episodes to decrease the randomness in actions\n",
    "            if episodes % 20 == 0 and episodes >= 21:\n",
    "              self.epsilon = max(self.epsilon_decay*self.epsilon,self.min_epsilon)\n",
    "            #achieved over 200 points which is the goal\n",
    "            if score >= 200:\n",
    "              print('score over 200')\n",
    "            #printing progress\n",
    "            if episodes % 100 == 0:\n",
    "              print('EPISODE {}, Average score = {}, Epsilon: {}'.format(episodes,np.mean(train_rewards[-100:]),self.epsilon))\n",
    "            #successful once reaches an average of over 200 points in the last 100 consecutive episodes\n",
    "            if np.mean(train_rewards[-100:]) >= 200  or episodes == 1000:\n",
    "              break\n",
    "        #env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run and train model\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print('start time: ',start)\n",
    "train_rewards = []\n",
    "NN = Train()\n",
    "NN.update(model,loss_fn,optimizer,train_rewards)\n",
    "end = time.time()\n",
    "print('end time: {} \\n time elapsed: {}'.format(end, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot to show reward per training episode\n",
    "episodes = range(len(train_rewards))\n",
    "avg = []\n",
    "for i in range(len(train_rewards)):\n",
    "    avg.append(np.mean(train_rewards[0:i]))\n",
    "    \n",
    "\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(episodes,train_rewards)\n",
    "plt.plot(episodes,avg, label='Running Average')\n",
    "plt.xlabel('Training Episode')\n",
    "plt.ylabel('Rewards Per Episode')\n",
    "plt.title('Rewards Per Training Episode')\n",
    "\n",
    "plt.savefig('rewards_per_episode.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model if needed\n",
    "torch.save(target_model.state_dict(), 'lunar_landing_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model if needed\n",
    "target_model = network(input_size, layer1_size, layer2_size, output_size)\n",
    "target_model.load_state_dict(torch.load('lunar_landing_model.pth'))\n",
    "target_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for reward per episode for 100 consecutive episodes using you trained agent\n",
    "episodes = range(100)\n",
    "\n",
    "test_rewards = []\n",
    "NN = Train()\n",
    "NN.test(episodes,target_model,loss_fn,test_rewards)\n",
    "\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(episodes,test_rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards Per Episode')\n",
    "plt.title('Rewards Per Episode With Trained Agent')\n",
    "\n",
    "plt.savefig('test_rewards_per_episode.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('hw3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f15868661e7856e58fef3293cd3a36c0a9f03c5ca222d55a991b439d72a8c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
